{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vec(emb_path, nmax=50000):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id\n",
    "\n",
    "# en-es.0-5000.txt     en-ru.0-5000.txt     fr-es.0-5000.txt     pt-de.0-5000.txt\n",
    "# bn-en.5000-6500.txt               en-es.5000-6500.txt\n",
    "\n",
    "def load_dict(dict_path=\"data/crosslingual/dictionaries/en-es.0-5000.txt\"):\n",
    "    \n",
    "    return pd.read_csv(dict_path, names=[\"src\", \"tgt\"], delim_whitespace=True)\n",
    "\n",
    "\n",
    "def multi_key_dict(words, dict_):\n",
    "    out = []\n",
    "    for word in words:\n",
    "        if word in dict_:\n",
    "            out.append(dict_[word])\n",
    "    return np.asarray(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_es_dict = load_dict()\n",
    "en_zh_dict = load_dict(\"data/crosslingual/dictionaries/en-zh.0-5000.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = 'data/wiki.en.vec'\n",
    "tgt_path = 'data/wiki.es.vec'\n",
    "tgt_path2 = 'data/wiki.zh.vec'\n",
    "nmax = 50000  # maximum number of word embeddings to load\n",
    "\n",
    "src_embeddings, src_id2word, src_word2id = load_vec(src_path, nmax)\n",
    "tgt_embeddings, tgt_id2word, tgt_word2id = load_vec(tgt_path, nmax)\n",
    "tgt_embeddings2, tgt_id2word2, tgt_word2id2 = load_vec(tgt_path2, nmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = ['data/wiki.en.vec', \n",
    "              'data/wiki.es.vec', \n",
    "              'data/wiki.zh.vec', \n",
    "              'data/wiki.ko.vec',\n",
    "              'data/wiki.ru.vec',\n",
    "              'data/wiki.ja.vec',\n",
    "              'data/wiki.de.vec',\n",
    "              'data/wiki.nl.vec',\n",
    "              'data/wiki.fr.vec',\n",
    "              'data/wiki.ar.vec']\n",
    "dictionaries = ['en-en.0-5000.txt',\n",
    "                'en-es.0-5000.txt',\n",
    "                'en-zh.0-5000.txt',\n",
    "                'en-ko.0-5000.txt',\n",
    "                'en-ru.0-5000.txt',\n",
    "                'en-ja.0-5000.txt',\n",
    "                'en-de.0-5000.txt',\n",
    "                'en-nl.0-5000.txt',\n",
    "                'en-fr.0-5000.txt',\n",
    "                'en-ar.0-5000.txt']\n",
    "languages = ['English', 'Spanish', 'Mandarin', 'Korean', 'Russian', 'Japanese', 'German', 'Dutch', 'French', 'Arabic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmax = 50000  # maximum number of word embeddings to load\n",
    "data = dict()\n",
    "for l_names, path, mydpath in zip(languages, embeddings, dictionaries):\n",
    "    emb, id2word, word2id = load_vec(path, nmax)\n",
    "    en_to_x_dict = load_dict(\"data/crosslingual/dictionaries/\" + mydpath)\n",
    "    src = en_to_x_dict[\"tgt\"].values\n",
    "    ids = multi_key_dict(src, word2id)\n",
    "    data[l_names] = emb[ids,:][:500,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "src, tgt, tgt2, = en_es_dict[\"src\"].values, en_es_dict[\"tgt\"].values, en_zh_dict[\"tgt\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_ids = multi_key_dict(src, src_word2id)\n",
    "tgt_ids = multi_key_dict(tgt, tgt_word2id)\n",
    "tgt_ids2 = multi_key_dict(tgt2, tgt_word2id2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11977, 300), (11374, 300), (8225, 300))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X , Y, Z = src_embeddings[src_ids, :], tgt_embeddings[tgt_ids,:], tgt_embeddings2[tgt_ids2,:]\n",
    "X.shape, Y.shape, Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ripser import Rips\n",
    "from persim import bottleneck, sliced_wasserstein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rips(maxdim=2, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
      "Language:  English\n",
      "Rips(maxdim=2, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
      "Language:  Spanish\n",
      "Rips(maxdim=2, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
      "Language:  Mandarin\n",
      "Rips(maxdim=2, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
      "Language:  Korean\n",
      "Rips(maxdim=2, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
      "Language:  Russian\n",
      "Rips(maxdim=2, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
      "Language:  Japanese\n",
      "Rips(maxdim=2, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
      "Language:  German\n",
      "Rips(maxdim=2, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
      "Language:  Dutch\n",
      "Rips(maxdim=2, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
      "Language:  French\n",
      "Rips(maxdim=2, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
      "Language:  Arabic\n"
     ]
    }
   ],
   "source": [
    "dgrms = dict()\n",
    "for language in languages:\n",
    "    rips = Rips(maxdim=2)\n",
    "    print(\"Language: \", language)\n",
    "    dgrms[language] = rips.fit_transform(data[language], metric=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree:\n",
    "    def __init__(self, data, left=None, right=None):\n",
    "        self.left = left \n",
    "        self.right = right\n",
    "        self.data = data\n",
    "    def __str__(self):\n",
    "        return \"Data: %s\" % self.data\n",
    "\n",
    "    @staticmethod\n",
    "    def node_list(root):\n",
    "        nlist = []\n",
    "        queue = [root]\n",
    "        while len(queue) > 0:\n",
    "            node = queue.pop()\n",
    "            if node.left:\n",
    "                queue.append(node.left)\n",
    "            if node.right:\n",
    "                queue.append(node.right)\n",
    "            else:\n",
    "                nlist.append(node)\n",
    "        return nlist\n",
    "    \n",
    "    def display(self, keys):\n",
    "        lines, *_ = self._display_aux(keys)\n",
    "        for line in lines:\n",
    "            print(line)\n",
    "\n",
    "    def _display_aux(self, keys):\n",
    "        \"\"\"Returns list of strings, width, height, and horizontal coordinate of the root.\"\"\"\n",
    "        # No child.\n",
    "        if self.right is None and self.left is None:\n",
    "            line = keys[self.data]\n",
    "            width = len(line)\n",
    "            height = 1\n",
    "            middle = width // 2\n",
    "            return [line], width, height, middle\n",
    "\n",
    "        # Only left child.\n",
    "        if self.right is None:\n",
    "            lines, n, p, x = self.left._display_aux(keys)\n",
    "            s = keys[self.data]\n",
    "            u = len(s)\n",
    "            first_line = (x + 1) * ' ' + (n - x - 1) * '_' + s\n",
    "            second_line = x * ' ' + '/' + (n - x - 1 + u) * ' '\n",
    "            shifted_lines = [line + u * ' ' for line in lines]\n",
    "            return [first_line, second_line] + shifted_lines, n + u, p + 2, n + u // 2\n",
    "\n",
    "        # Only right child.\n",
    "        if self.left is None:\n",
    "            lines, n, p, x = self.right._display_aux(keys)\n",
    "            s = keys[self.data]\n",
    "            u = len(s)\n",
    "            first_line = s + x * '_' + (n - x) * ' '\n",
    "            second_line = (u + x) * ' ' + '\\\\' + (n - x - 1) * ' '\n",
    "            shifted_lines = [u * ' ' + line for line in lines]\n",
    "            return [first_line, second_line] + shifted_lines, n + u, p + 2, u // 2\n",
    "\n",
    "        # Two children.\n",
    "        left, n, p, x = self.left._display_aux(keys)\n",
    "        right, m, q, y = self.right._display_aux(keys)\n",
    "        s = keys[self.data]\n",
    "        u = len(s)+1\n",
    "        first_line = (x + 1) * ' ' + (n - x - 1) * '_' + s + y * '_' + (m - y) * ' '\n",
    "        second_line = x * ' ' + '/' + (n - x - 1 + u + y) * ' ' + '\\\\' + (m - y - 1) * ' '\n",
    "        if p < q:\n",
    "            left += [n * ' '] * (q - p)\n",
    "        elif q < p:\n",
    "            right += [m * ' '] * (p - q)\n",
    "        zipped_lines = zip(left, right)\n",
    "        lines = [first_line, second_line] + [a + u * ' ' + b for a, b in zipped_lines]\n",
    "        return lines, n + m + u, max(p, q) + 2, n + u // 2\n",
    "\n",
    "\n",
    "def distance_matrix(nodes, tiny_dmatrix):\n",
    "    \"\"\"\n",
    "    Computes distance matrix with complete linkage \n",
    "    \"\"\"\n",
    "    d_matrix = np.full((len(nodes), len(nodes)), np.nan)\n",
    "    for i in range(len(nodes)):\n",
    "        for j in range(i+1, len(nodes)):\n",
    "            nlist_1 = Tree.node_list(nodes[i])\n",
    "            nlist_2 = Tree.node_list(nodes[j])\n",
    "            ids_1 = [node.data for node in nlist_1]\n",
    "            ids_2 = [node.data for node in nlist_2]\n",
    "            d_matrix[i,j] = max([tiny_dmatrix[i,j] for i in ids_1 for j in ids_2])\n",
    "            d_matrix[i,j] = max([tiny_dmatrix[i,j] for i in ids_1 for j in ids_2])\n",
    "#             if isinstance(nodes[i], Tree) and not isinstance(nodes[j], Tree):\n",
    "#                 nlist = Tree.node_list(nodes[i])\n",
    "#                 ids = [node.data for node in nlist]\n",
    "#                 d_matrix[i,j] = max([tiny_dmatrix[i,j] for i in ids])\n",
    "# #                 d_matrix[i,j] = min([tiny_dmatrix[i,j] for i in ids for j in ids])\n",
    "#             elif isinstance(nodes[j], Tree) and not isinstance(nodes[i], Tree): \n",
    "#                 nlist = Tree.node_list(nodes[j])\n",
    "#                 ids = [node.data for node in nlist]\n",
    "# #                 d_matrix[i,j] = max([tiny_dmatrix[i,j] for i in ids for j in ids])\n",
    "#                 d_matrix[i,j] = min([tiny_dmatrix[i,j] for i in ids for j in ids])\n",
    "#             elif isinstance(nodes[i], Tree) and isinstance(nodes[j], Tree):\n",
    "                \n",
    "#             else: \n",
    "#                 d_matrix[i,j] = tiny_dmatrix[nodes[i].data, nodes[j].data]\n",
    "    return d_matrix\n",
    "    \n",
    "def hclustering(dgrms, homology=0, dist='sw'):\n",
    "    nodes = [Tree(i) for i in range(len(dgrms))]\n",
    "    new_dgrms = [dgrms[i][homology] for i in dgrms]\n",
    "    tiny_dmatrix = np.full((len(nodes), len(nodes)), np.nan)                \n",
    "    for i in range(len(nodes)):\n",
    "        for j in range(i+1, len(nodes)):\n",
    "            if dist == 'sw':\n",
    "                tiny_dmatrix[i,j] = sliced_wasserstein(new_dgrms[nodes[i].data], new_dgrms[nodes[j].data])\n",
    "            else:\n",
    "                tiny_dmatrix[i,j] = bottleneck(new_dgrms[nodes[i].data], new_dgrms[nodes[j].data])\n",
    "            tiny_dmatrix[j,i] = tiny_dmatrix[i,j]\n",
    "#     langs = list(dgrms.keys())\n",
    "#     langs.append('')\n",
    "    while len(nodes) > 1:\n",
    "#         a =[print(node) for node in nodes]\n",
    "#         a = [node.display(langs) for node in nodes]\n",
    "        d_matrix = distance_matrix(nodes, tiny_dmatrix)\n",
    "#         print(d_matrix)\n",
    "        i, j = np.unravel_index(np.nanargmin(d_matrix), d_matrix.shape)\n",
    "        print(\"The minimum is \", d_matrix[i,j])\n",
    "        node = Tree(-1, left=nodes[i], right=nodes[j])\n",
    "        nodes = [nodes[k] for k in range(len(nodes)) if k not in [i,j]]\n",
    "        nodes.append(node)        \n",
    "    return nodes[0], tiny_dmatrix\n",
    "    \n",
    "def hclustering_all(dgrms, dist='sw'):\n",
    "    nodes = [Tree(i) for i in range(len(dgrms))]\n",
    "#     new_dgrms = [dgrms[i][homology] for i in dgrms]\n",
    "    new_dgrms = [np.vstack((dgrms[i][0], dgrms[i][1], dgrms[i][2])) for i in dgrms]\n",
    "    tiny_dmatrix = np.full((len(nodes), len(nodes)), np.nan)                \n",
    "    for i in range(len(nodes)):\n",
    "        for j in range(i+1, len(nodes)):\n",
    "            if dist == 'sw':\n",
    "                tiny_dmatrix[i,j] = sliced_wasserstein(new_dgrms[nodes[i].data], new_dgrms[nodes[j].data])\n",
    "            else:\n",
    "                tiny_dmatrix[i,j] = bottleneck(new_dgrms[nodes[i].data], new_dgrms[nodes[j].data])\n",
    "            tiny_dmatrix[j,i] = tiny_dmatrix[i,j]\n",
    "    langs = list(dgrms.keys())\n",
    "    langs.append('')\n",
    "    while len(nodes) > 1:\n",
    "#         a =[print(node) for node in nodes]\n",
    "        a = [node.display(langs) for node in nodes]\n",
    "        d_matrix = distance_matrix(nodes, tiny_dmatrix, new_dgrms)\n",
    "        i, j = np.unravel_index(np.nanargmin(d_matrix), d_matrix.shape)\n",
    "        print(\"The minimum is \", d_matrix[i,j])\n",
    "        node = Tree(None, left=nodes[i], right=nodes[j])\n",
    "        nodes = [nodes[k] for k in range(len(nodes)) if k not in [i,j]]\n",
    "        nodes.append(node)        \n",
    "    return nodes[0], tiny_dmatrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q_, tiny = hclustering_all(dgrms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = list(dgrms.keys())\n",
    "langs.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum is  0.006297610371318185\n",
      "The minimum is  1.029602312725588\n",
      "The minimum is  1.2852748297401468\n",
      "The minimum is  1.5694235805241241\n",
      "The minimum is  2.2696271079093346\n",
      "The minimum is  3.0640762510677306\n",
      "The minimum is  4.8399114986486085\n",
      "The minimum is  7.33214264187133\n",
      "The minimum is  10.27542890036365\n"
     ]
    }
   ],
   "source": [
    "q1, tiny = hclustering(dgrms,homology=2,dist='sw');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in languages:\n",
    "    plt.figure()\n",
    "    rips.plot(dgrms[lang])\n",
    "    plt.title(lang)\n",
    "    plt.xlim([0,1])\n",
    "    plt.ylim([0,1])\n",
    "    plt.savefig('pds/' + lang + '.png', dpi=600, bbox_inches = 'tight', pad_inches = 0.2)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ________                                                              \n",
      "   /         \\                                                             \n",
      "Korean     ___________________                                            \n",
      "          /                    \\                                           \n",
      "       Arabic          _______________                                    \n",
      "                      /                \\                                   \n",
      "                   _______         ________________________              \n",
      "                  /        \\       /                         \\             \n",
      "              Mandarin Japanese Russian         ____________________      \n",
      "                                               /                     \\     \n",
      "                                            _________            _____   \n",
      "                                           /          \\          /      \\  \n",
      "                                        Spanish     _____    English Dutch\n",
      "                                                   /      \\                \n",
      "                                                German French              \n"
     ]
    }
   ],
   "source": [
    "q1.display(langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_latex_tree(tree,langs):\n",
    "    print(chr(92)+'begin{forest}\\n[',end='')\n",
    "    _make_latex_tree(tree,langs)\n",
    "    print(']\\n'+ chr(92) +'end{forest}',end='')\n",
    "\n",
    "def _make_latex_tree(tree, langs):\n",
    "    if not tree.left and not tree.right:\n",
    "        print(langs[tree.data], end='')\n",
    "    elif tree.data:\n",
    "        print('|[', end='')\n",
    "        _make_latex_tree(tree.left,langs)\n",
    "        print(']',end='')\n",
    "        print('[',end='')\n",
    "        _make_latex_tree(tree.right,langs)\n",
    "        print(']',end='')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{forest}\n",
      "[|[Korean][|[Arabic][|[|[Mandarin][Japanese]][|[Russian][|[|[Spanish][|[German][French]]][|[English][Dutch]]]]]]]\n",
      "\\end{forest}"
     ]
    }
   ],
   "source": [
    "make_latex_tree(q1,langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
