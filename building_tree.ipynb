{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vec(emb_path, nmax=50000):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id\n",
    "\n",
    "def load_dict(dict_path=\"data/crosslingual/dictionaries/en-es.0-5000.txt\"):\n",
    "    \n",
    "    return pd.read_csv(dict_path, names=[\"src\", \"tgt\"], delim_whitespace=True)\n",
    "\n",
    "\n",
    "def multi_key_dict(words, dict_):\n",
    "    out = []\n",
    "    for word in words:\n",
    "        if word in dict_:\n",
    "            out.append(dict_[word])\n",
    "    return np.asarray(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = ['data/wiki.en.vec', \n",
    "              'data/wiki.es.vec', \n",
    "              'data/wiki.zh.vec', \n",
    "              'data/wiki.ko.vec',\n",
    "              'data/wiki.ru.vec',\n",
    "              'data/wiki.ja.vec',\n",
    "              'data/wiki.de.vec',\n",
    "              'data/wiki.nl.vec',\n",
    "              'data/wiki.fr.vec',\n",
    "              'data/wiki.ar.vec',\n",
    "              'data/wiki.fi.vec',\n",
    "              'data/wiki.hu.vec']\n",
    "dictionaries = ['en-en.0-5000.txt',\n",
    "                'en-es.0-5000.txt',\n",
    "                'en-zh.0-5000.txt',\n",
    "                'en-ko.0-5000.txt',\n",
    "                'en-ru.0-5000.txt',\n",
    "                'en-ja.0-5000.txt',\n",
    "                'en-de.0-5000.txt',\n",
    "                'en-nl.0-5000.txt',\n",
    "                'en-fr.0-5000.txt',\n",
    "                'en-ar.0-5000.txt',\n",
    "                'en-fi.0-5000.txt',\n",
    "                'en-hu.0-5000.txt']\n",
    "languages = ['English', 'Spanish', 'Mandarin', 'Korean', 'Russian', 'Japanese', 'German', 'Dutch', 'French', 'Arabic', 'Finnish', 'Hungarian']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmax = 50000  # maximum number of word embeddings to load\n",
    "data = dict()\n",
    "for l_names, path, mydpath in zip(languages, embeddings, dictionaries):\n",
    "    emb, id2word, word2id = load_vec(path, nmax)\n",
    "    en_to_x_dict = load_dict(\"data/crosslingual/dictionaries/\" + mydpath)\n",
    "    src = en_to_x_dict[\"tgt\"].values\n",
    "    ids = multi_key_dict(src, word2id)\n",
    "    data[l_names] = emb[ids,:][:500,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ripser import Rips\n",
    "from persim import bottleneck, sliced_wasserstein\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_dist(a,b):\n",
    "    sim = cosine(a,b)\n",
    "    return np.arccos(1.0 - sim)/np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rips(maxdim=2, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
      "Language:  English\n",
      "Rips(maxdim=2, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
      "Language:  Spanish\n",
      "Rips(maxdim=2, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
      "Language:  Mandarin\n",
      "Rips(maxdim=2, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
      "Language:  Korean\n",
      "Rips(maxdim=2, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
      "Language:  Russian\n",
      "Rips(maxdim=2, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
      "Language:  Japanese\n",
      "Rips(maxdim=2, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
      "Language:  German\n",
      "Rips(maxdim=2, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
      "Language:  Dutch\n",
      "Rips(maxdim=2, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
      "Language:  French\n",
      "Rips(maxdim=2, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
      "Language:  Arabic\n",
      "Rips(maxdim=2, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
      "Language:  Finnish\n",
      "Rips(maxdim=2, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
      "Language:  Hungarian\n"
     ]
    }
   ],
   "source": [
    "dgrms = dict()\n",
    "for language in languages:\n",
    "    rips = Rips(maxdim=2)\n",
    "    print(\"Language: \", language)\n",
    "    dgrms[language] = rips.fit_transform(data[language], metric=cosine_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in languages:\n",
    "    plt.figure()\n",
    "    rips.plot(dgrms[lang])\n",
    "    plt.title(lang)\n",
    "#     plt.xlim([0,1])\n",
    "#     plt.ylim([0,1])\n",
    "    plt.savefig('pds_euc/' + lang + '.png', dpi=600, bbox_inches = 'tight', pad_inches = 0.2)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree:\n",
    "    def __init__(self, data, left=None, right=None):\n",
    "        self.left = left \n",
    "        self.right = right\n",
    "        self.data = data\n",
    "    def __str__(self):\n",
    "        return \"Data: %s\" % self.data\n",
    "\n",
    "    @staticmethod\n",
    "    def node_list(root):\n",
    "        nlist = []\n",
    "        queue = [root]\n",
    "        while len(queue) > 0:\n",
    "            node = queue.pop()\n",
    "            if node.left:\n",
    "                queue.append(node.left)\n",
    "            if node.right:\n",
    "                queue.append(node.right)\n",
    "            else:\n",
    "                nlist.append(node)\n",
    "        return nlist\n",
    "    \n",
    "    def display(self, keys):\n",
    "        lines, *_ = self._display_aux(keys)\n",
    "        for line in lines:\n",
    "            print(line)\n",
    "\n",
    "    def _display_aux(self, keys):\n",
    "        \"\"\"Returns list of strings, width, height, and horizontal coordinate of the root.\"\"\"\n",
    "        # No child.\n",
    "        if self.right is None and self.left is None:\n",
    "            line = keys[self.data]\n",
    "            width = len(line)\n",
    "            height = 1\n",
    "            middle = width // 2\n",
    "            return [line], width, height, middle\n",
    "\n",
    "        # Only left child.\n",
    "        if self.right is None:\n",
    "            lines, n, p, x = self.left._display_aux(keys)\n",
    "            s = keys[self.data]\n",
    "            u = len(s)\n",
    "            first_line = (x + 1) * ' ' + (n - x - 1) * '_' + s\n",
    "            second_line = x * ' ' + '/' + (n - x - 1 + u) * ' '\n",
    "            shifted_lines = [line + u * ' ' for line in lines]\n",
    "            return [first_line, second_line] + shifted_lines, n + u, p + 2, n + u // 2\n",
    "\n",
    "        # Only right child.\n",
    "        if self.left is None:\n",
    "            lines, n, p, x = self.right._display_aux(keys)\n",
    "            s = keys[self.data]\n",
    "            u = len(s)\n",
    "            first_line = s + x * '_' + (n - x) * ' '\n",
    "            second_line = (u + x) * ' ' + '\\\\' + (n - x - 1) * ' '\n",
    "            shifted_lines = [u * ' ' + line for line in lines]\n",
    "            return [first_line, second_line] + shifted_lines, n + u, p + 2, u // 2\n",
    "\n",
    "        # Two children.\n",
    "        left, n, p, x = self.left._display_aux(keys)\n",
    "        right, m, q, y = self.right._display_aux(keys)\n",
    "        s = keys[self.data]\n",
    "        u = len(s)+1\n",
    "        first_line = (x + 1) * ' ' + (n - x - 1) * '_' + s + y * '_' + (m - y) * ' '\n",
    "        second_line = x * ' ' + '/' + (n - x - 1 + u + y) * ' ' + '\\\\' + (m - y - 1) * ' '\n",
    "        if p < q:\n",
    "            left += [n * ' '] * (q - p)\n",
    "        elif q < p:\n",
    "            right += [m * ' '] * (p - q)\n",
    "        zipped_lines = zip(left, right)\n",
    "        lines = [first_line, second_line] + [a + u * ' ' + b for a, b in zipped_lines]\n",
    "        return lines, n + m + u, max(p, q) + 2, n + u // 2\n",
    "\n",
    "\n",
    "def distance_matrix(nodes, tiny_dmatrix):\n",
    "    \"\"\"\n",
    "    Computes distance matrix with complete linkage \n",
    "    \"\"\"\n",
    "    d_matrix = np.full((len(nodes), len(nodes)), np.nan)\n",
    "    for i in range(len(nodes)):\n",
    "        for j in range(i+1, len(nodes)):\n",
    "            nlist_1 = Tree.node_list(nodes[i])\n",
    "            nlist_2 = Tree.node_list(nodes[j])\n",
    "            ids_1 = [node.data for node in nlist_1]\n",
    "            ids_2 = [node.data for node in nlist_2]\n",
    "            d_matrix[i,j] = min([tiny_dmatrix[i,j] for i in ids_1 for j in ids_2])\n",
    "#             d_matrix[i,j] = max([tiny_dmatrix[i,j] for i in ids_1 for j in ids_2])\n",
    "    return d_matrix\n",
    "    \n",
    "def hclustering(dgrms, homology=0, dist='sw'):\n",
    "    nodes = [Tree(i) for i in range(len(dgrms))]\n",
    "    if type(homology) is int:\n",
    "        new_dgrms = [dgrms[i][homology] for i in dgrms]\n",
    "    elif len(homology) == 2:\n",
    "        new_dgrms = [np.vstack((dgrms[i][homology[0]], dgrms[i][homology[1]])) for i in dgrms]\n",
    "    tiny_dmatrix = np.full((len(nodes), len(nodes)), np.nan)                \n",
    "    for i in range(len(nodes)):\n",
    "        for j in range(i+1, len(nodes)):\n",
    "            if dist == 'sw':\n",
    "                tiny_dmatrix[i,j] = sliced_wasserstein(new_dgrms[nodes[i].data], new_dgrms[nodes[j].data])\n",
    "            else:\n",
    "                tiny_dmatrix[i,j] = bottleneck(new_dgrms[nodes[i].data], new_dgrms[nodes[j].data])\n",
    "            tiny_dmatrix[j,i] = tiny_dmatrix[i,j]\n",
    "    while len(nodes) > 1:\n",
    "        d_matrix = distance_matrix(nodes, tiny_dmatrix)\n",
    "        i, j = np.unravel_index(np.nanargmin(d_matrix), d_matrix.shape)\n",
    "        print(\"The minimum is \", d_matrix[i,j])\n",
    "        node = Tree(-1, left=nodes[i], right=nodes[j])\n",
    "        nodes = [nodes[k] for k in range(len(nodes)) if k not in [i,j]]\n",
    "        nodes.append(node)        \n",
    "    return nodes[0], tiny_dmatrix\n",
    "    \n",
    "def hclustering_all(dgrms, dist='sw'):\n",
    "    nodes = [Tree(i) for i in range(len(dgrms))]\n",
    "#     new_dgrms = [dgrms[i][homology] for i in dgrms]\n",
    "    new_dgrms = [np.vstack((dgrms[i][0], dgrms[i][1], dgrms[i][2])) for i in dgrms]\n",
    "    tiny_dmatrix = np.full((len(nodes), len(nodes)), np.nan)                \n",
    "    for i in range(len(nodes)):\n",
    "        for j in range(i+1, len(nodes)):\n",
    "            if dist == 'sw':\n",
    "                tiny_dmatrix[i,j] = sliced_wasserstein(new_dgrms[nodes[i].data], new_dgrms[nodes[j].data])\n",
    "            else:\n",
    "                tiny_dmatrix[i,j] = bottleneck(new_dgrms[nodes[i].data], new_dgrms[nodes[j].data])\n",
    "            tiny_dmatrix[j,i] = tiny_dmatrix[i,j]\n",
    "    langs = list(dgrms.keys())\n",
    "    langs.append('')\n",
    "    while len(nodes) > 1:\n",
    "#         a =[print(node) for node in nodes]\n",
    "        a = [node.display(langs) for node in nodes]\n",
    "        d_matrix = distance_matrix(nodes, tiny_dmatrix, new_dgrms)\n",
    "        i, j = np.unravel_index(np.nanargmin(d_matrix), d_matrix.shape)\n",
    "        print(\"The minimum is \", d_matrix[i,j])\n",
    "        node = Tree(None, left=nodes[i], right=nodes[j])\n",
    "        nodes = [nodes[k] for k in range(len(nodes)) if k not in [i,j]]\n",
    "        nodes.append(node)        \n",
    "    return nodes[0], tiny_dmatrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q_, tiny = hclustering_all(dgrms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = list(dgrms.keys())\n",
    "langs.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum is  3.5841900478915716\n",
      "The minimum is  3.889170751648887\n",
      "The minimum is  3.9964367175133817\n",
      "The minimum is  4.193152133248779\n",
      "The minimum is  4.418000561126398\n",
      "The minimum is  4.530739646322655\n",
      "The minimum is  4.76305433324643\n",
      "The minimum is  5.303667859652638\n",
      "The minimum is  5.921956987773676\n",
      "The minimum is  6.148572993379346\n",
      "The minimum is  6.970068008775321\n"
     ]
    }
   ],
   "source": [
    "q1, tiny = hclustering(dgrms,homology=1,dist='sw');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    _________                                                                               \n",
      "   /          \\                                                                              \n",
      "Russian     ________________                                                                \n",
      "           /                 \\                                                               \n",
      "        German         ______________                                                       \n",
      "                      /               \\                                                      \n",
      "                   ______         __________________________________________               \n",
      "                  /       \\       /                                           \\              \n",
      "               Spanish French Mandarin        ______________________________________        \n",
      "                                             /                                       \\       \n",
      "                                           ___________                            _____    \n",
      "                                          /            \\                          /      \\   \n",
      "                                       Arabic      _________                  Korean Finnish\n",
      "                                                  /          \\                               \n",
      "                                              Hungarian    _________                        \n",
      "                                                          /          \\                       \n",
      "                                                        Dutch     _______                   \n",
      "                                                                 /        \\                  \n",
      "                                                              English Japanese               \n"
     ]
    }
   ],
   "source": [
    "q1.display(langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_latex_tree(tree,langs):\n",
    "    print(chr(92)+'begin{forest}\\n[',end='')\n",
    "    _make_latex_tree(tree,langs)\n",
    "    print(']\\n'+ chr(92) +'end{forest}',end='')\n",
    "\n",
    "def _make_latex_tree(tree, langs):\n",
    "    if not tree.left and not tree.right:\n",
    "        print(langs[tree.data], end='')\n",
    "    elif tree.data:\n",
    "        print('|[', end='')\n",
    "        _make_latex_tree(tree.left,langs)\n",
    "        print(']',end='')\n",
    "        print('[',end='')\n",
    "        _make_latex_tree(tree.right,langs)\n",
    "        print(']',end='')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{forest}\n",
      "[|[Russian][|[German][|[|[Spanish][French]][|[Mandarin][|[|[Arabic][|[Hungarian][|[Dutch][|[English][Japanese]]]]][|[Korean][Finnish]]]]]]]\n",
      "\\end{forest}"
     ]
    }
   ],
   "source": [
    "make_latex_tree(q1,langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
