{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vec(emb_path, nmax=50000):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id\n",
    "\n",
    "# en-es.0-5000.txt     en-ru.0-5000.txt     fr-es.0-5000.txt     pt-de.0-5000.txt\n",
    "# bn-en.5000-6500.txt               en-es.5000-6500.txt\n",
    "\n",
    "def load_dict(dict_path=\"data/crosslingual/dictionaries/en-es.0-5000.txt\"):\n",
    "    \n",
    "    return pd.read_csv(dict_path, names=[\"src\", \"tgt\"], delim_whitespace=True)\n",
    "\n",
    "\n",
    "def multi_key_dict(words, dict_):\n",
    "    out = []\n",
    "    for word in words:\n",
    "        if word in dict_:\n",
    "            out.append(dict_[word])\n",
    "    return np.asarray(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_es_dict = load_dict()\n",
    "en_zh_dict = load_dict(\"data/crosslingual/dictionaries/en-zh.0-5000.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = 'data/wiki.en.vec'\n",
    "tgt_path = 'data/wiki.es.vec'\n",
    "tgt_path2 = 'data/wiki.zh.vec'\n",
    "nmax = 50000  # maximum number of word embeddings to load\n",
    "\n",
    "src_embeddings, src_id2word, src_word2id = load_vec(src_path, nmax)\n",
    "tgt_embeddings, tgt_id2word, tgt_word2id = load_vec(tgt_path, nmax)\n",
    "tgt_embeddings2, tgt_id2word2, tgt_word2id2 = load_vec(tgt_path2, nmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "src, tgt, tgt2, = en_es_dict[\"src\"].values, en_es_dict[\"tgt\"].values, en_zh_dict[\"tgt\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_ids = multi_key_dict(src, src_word2id)\n",
    "tgt_ids = multi_key_dict(tgt, tgt_word2id)\n",
    "tgt_ids2 = multi_key_dict(tgt2, tgt_word2id2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11977, 300), (11374, 300), (8225, 300))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X , Y, Z = src_embeddings[src_ids, :], tgt_embeddings[tgt_ids,:], tgt_embeddings2[tgt_ids2,:]\n",
    "X.shape, Y.shape, Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ripser import Rips\n",
    "from persim import bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rips(maxdim=2, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
      "Rips(maxdim=2, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
      "Rips(maxdim=2, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n"
     ]
    }
   ],
   "source": [
    "dgrms = dict()\n",
    "dsets = [X, Y, Z]\n",
    "i=0\n",
    "for language in ['English', 'Spanish', 'Mandarin']:\n",
    "    rips = Rips(maxdim=2)\n",
    "    data = dsets[i][:500,:]\n",
    "    diagrams = rips.fit_transform(data, metric=\"cosine\")\n",
    "    dgrms[language] = diagrams\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree:\n",
    "    def __init__(self, data, left=None, right=None):\n",
    "        self.left = left \n",
    "        self.right = right\n",
    "        self.data = data\n",
    "    def __str__(self):\n",
    "        return \"Data: %s\" % self.data\n",
    "\n",
    "    @staticmethod\n",
    "    def node_list(root):\n",
    "        nlist = []\n",
    "        queue = [root]\n",
    "        while len(queue) > 0:\n",
    "            node = queue.pop()\n",
    "            if node.left:\n",
    "                queue.append(node.left)\n",
    "            if node.right:\n",
    "                queue.append(node.right)\n",
    "            else:\n",
    "                nlist.append(node)\n",
    "        return nlist\n",
    "    \n",
    "    def display(self, keys):\n",
    "        lines, *_ = self._display_aux(keys)\n",
    "        for line in lines:\n",
    "            print(line)\n",
    "\n",
    "    def _display_aux(self, keys):\n",
    "        \"\"\"Returns list of strings, width, height, and horizontal coordinate of the root.\"\"\"\n",
    "        # No child.\n",
    "        if self.right is None and self.left is None:\n",
    "            line = '%s' % keys[self.data]\n",
    "            width = len(line)\n",
    "            height = 1\n",
    "            middle = width // 2\n",
    "            return [line], width, height, middle\n",
    "\n",
    "        # Only left child.\n",
    "        if self.right is None:\n",
    "            lines, n, p, x = self.left._display_aux()\n",
    "            s = '%s' % keys[self.data]\n",
    "            u = len(s)\n",
    "            first_line = (x + 1) * ' ' + (n - x - 1) * '_' + s\n",
    "            second_line = x * ' ' + '/' + (n - x - 1 + u) * ' '\n",
    "            shifted_lines = [line + u * ' ' for line in lines]\n",
    "            return [first_line, second_line] + shifted_lines, n + u, p + 2, n + u // 2\n",
    "\n",
    "        # Only right child.\n",
    "        if self.left is None:\n",
    "            lines, n, p, x = self.right._display_aux()\n",
    "            s = '%s' % keys[self.data]\n",
    "            u = len(s)\n",
    "            first_line = s + x * '_' + (n - x) * ' '\n",
    "            second_line = (u + x) * ' ' + '\\\\' + (n - x - 1) * ' '\n",
    "            shifted_lines = [u * ' ' + line for line in lines]\n",
    "            return [first_line, second_line] + shifted_lines, n + u, p + 2, u // 2\n",
    "\n",
    "        # Two children.\n",
    "        left, n, p, x = self.left._display_aux()\n",
    "        right, m, q, y = self.right._display_aux()\n",
    "        s = '%s' % keys[self.data]\n",
    "        u = len(s)\n",
    "        first_line = (x + 1) * ' ' + (n - x - 1) * '_' + s + y * '_' + (m - y) * ' '\n",
    "        second_line = x * ' ' + '/' + (n - x - 1 + u + y) * ' ' + '\\\\' + (m - y - 1) * ' '\n",
    "        if p < q:\n",
    "            left += [n * ' '] * (q - p)\n",
    "        elif q < p:\n",
    "            right += [m * ' '] * (p - q)\n",
    "        zipped_lines = zip(left, right)\n",
    "        lines = [first_line, second_line] + [a + u * ' ' + b for a, b in zipped_lines]\n",
    "        return lines, n + m + u, max(p, q) + 2, n + u // 2\n",
    "\n",
    "\n",
    "def distance_matrix(nodes, tiny_dmatrix, dgrms):\n",
    "    \"\"\"\n",
    "    Computes distance matrix with complete linkage \n",
    "    \"\"\"\n",
    "    d_matrix = np.full((len(nodes), len(nodes)), np.nan)\n",
    "    for i in range(len(nodes)):\n",
    "        for j in range(i+1, len(nodes)):\n",
    "            if isinstance(nodes[i], Tree) and not isinstance(nodes[j], Tree):\n",
    "                nlist = Tree.node_list(nodes[i])\n",
    "                ids = [node.data for node in nlist]\n",
    "                d_matrix[i,j] = max([[tiny_dmatrix[i,j] for i in ids] for j in ids])\n",
    "            elif isinstance(nodes[j], Tree) and not isinstance(nodes[i], Tree): \n",
    "                nlist = Tree.node_list(nodes[j])\n",
    "                ids = [node.data for node in nlist]\n",
    "                d_matrix[i,j] = max([[tiny_dmatrix[i,j] for i in ids] for j in ids])\n",
    "            elif isinstance(nodes[i], Tree) and isinstance(nodes[j], Tree):\n",
    "                nlist_1 = Tree.node_list(nodes[i])\n",
    "                nlist_2 = Tree.node_list(nodes[j])\n",
    "                ids_1 = [node.data for node in nlist_1]\n",
    "                ids_2 = [node.data for node in nlist_2]\n",
    "                d_matrix[i,j] = max([tiny_dmatrix[i,j] for i in ids_1 for j in ids_2])\n",
    "            else: \n",
    "                d_matrix[i,j] = bottleneck(dgrms[nodes[i].data], dgrms[nodes[j].data])\n",
    "            d_matrix[j,i] = d_matrix[i,j]\n",
    "        return d_matrix\n",
    "    \n",
    "def hclustering(dgrms, homology=0):\n",
    "    nodes = [Tree(i) for i in range(len(dgrms))]\n",
    "    new_dgrms = [dgrms[i][homology] for i in dgrms]\n",
    "    tiny_dmatrix = np.full((len(nodes), len(nodes)), np.nan)                \n",
    "    for i in range(len(nodes)):\n",
    "        for j in range(i+1, len(nodes)):\n",
    "            tiny_dmatrix[i,j] = bottleneck(new_dgrms[nodes[i].data], new_dgrms[nodes[j].data])\n",
    "    while len(nodes) > 1:\n",
    "        a =[print(node) for node in nodes]\n",
    "        d_matrix = distance_matrix(nodes, tiny_dmatrix, new_dgrms)\n",
    "        i, j = np.unravel_index(np.argmin(d_matrix), d_matrix.shape)\n",
    "        node = Tree(None, left=nodes.pop(i), right=nodes.pop(j))\n",
    "        nodes.append(node)        \n",
    "    return nodes[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: 0, Left Node: None, Right Node: None\n",
      "\n",
      "Data: 1, Left Node: None, Right Node: None\n",
      "\n",
      "Data: 2, Left Node: None, Right Node: None\n",
      "\n",
      "Data: 2, Left Node: None, Right Node: None\n",
      "\n",
      "Data: None, Left Node: Data: 0, Left Node: None, Right Node: None\n",
      ", Right Node: Data: 1, Left Node: None, Right Node: None\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = hclustering(dgrms,homology=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['English', 'Spanish', 'Mandarin'])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.display(dgrms.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
