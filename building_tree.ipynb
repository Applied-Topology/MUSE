{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vec(emb_path, nmax=50000):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id\n",
    "\n",
    "def load_dict(dict_path=\"data/crosslingual/dictionaries/en-es.0-5000.txt\"):\n",
    "    \n",
    "    return pd.read_csv(dict_path, names=[\"src\", \"tgt\"], delim_whitespace=True)\n",
    "\n",
    "\n",
    "def multi_key_dict(words, dict_):\n",
    "    out = []\n",
    "    for word in words:\n",
    "        if word in dict_:\n",
    "            out.append(dict_[word])\n",
    "    return np.asarray(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = ['data/wiki.en.vec', \n",
    "              'data/wiki.es.vec', \n",
    "              'data/wiki.zh.vec', \n",
    "              'data/wiki.ko.vec',\n",
    "              'data/wiki.ru.vec',\n",
    "              'data/wiki.ja.vec',\n",
    "              'data/wiki.de.vec',\n",
    "              'data/wiki.nl.vec',\n",
    "              'data/wiki.fr.vec',\n",
    "              'data/wiki.ar.vec',\n",
    "              'data/wiki.fi.vec',\n",
    "              'data/wiki.hu.vec']\n",
    "dictionaries = ['en-en.0-5000.txt',\n",
    "                'en-es.0-5000.txt',\n",
    "                'en-zh.0-5000.txt',\n",
    "                'en-ko.0-5000.txt',\n",
    "                'en-ru.0-5000.txt',\n",
    "                'en-ja.0-5000.txt',\n",
    "                'en-de.0-5000.txt',\n",
    "                'en-nl.0-5000.txt',\n",
    "                'en-fr.0-5000.txt',\n",
    "                'en-ar.0-5000.txt',\n",
    "                'en-fi.0-5000.txt',\n",
    "                'en-hu.0-5000.txt']\n",
    "languages = ['English', 'Spanish', 'Mandarin', 'Korean', 'Russian', 'Japanese', 'German', 'Dutch', 'French', 'Arabic', 'Finnish', 'Hungarian']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmax = 50000  # maximum number of word embeddings to load\n",
    "data = dict()\n",
    "for l_names, path, mydpath in zip(languages, embeddings, dictionaries):\n",
    "    emb, id2word, word2id = load_vec(path, nmax)\n",
    "    en_to_x_dict = load_dict(\"data/crosslingual/dictionaries/\" + mydpath)\n",
    "    src = en_to_x_dict[\"tgt\"].values\n",
    "    ids = multi_key_dict(src, word2id)\n",
    "    data[l_names] = emb[ids,:][:200,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ripser import Rips\n",
    "from persim import bottleneck, sliced_wasserstein\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_dist(a,b):\n",
    "    sim = cosine(a,b)\n",
    "    return np.arccos(1.0 - sim)/np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rips(maxdim=3, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
      "Language:  English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/ripser/ripser.py:257: UserWarning: The input point cloud has more columns than rows; did you mean to transpose?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rips(maxdim=3, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
      "Language:  Spanish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/ripser/ripser.py:257: UserWarning: The input point cloud has more columns than rows; did you mean to transpose?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rips(maxdim=3, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
      "Language:  Mandarin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/ripser/ripser.py:257: UserWarning: The input point cloud has more columns than rows; did you mean to transpose?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rips(maxdim=3, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
      "Language:  Korean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/ripser/ripser.py:257: UserWarning: The input point cloud has more columns than rows; did you mean to transpose?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rips(maxdim=3, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
      "Language:  Russian\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/ripser/ripser.py:257: UserWarning: The input point cloud has more columns than rows; did you mean to transpose?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rips(maxdim=3, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
      "Language:  Japanese\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/ripser/ripser.py:257: UserWarning: The input point cloud has more columns than rows; did you mean to transpose?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rips(maxdim=3, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
      "Language:  German\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/ripser/ripser.py:257: UserWarning: The input point cloud has more columns than rows; did you mean to transpose?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rips(maxdim=3, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
      "Language:  Dutch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/ripser/ripser.py:257: UserWarning: The input point cloud has more columns than rows; did you mean to transpose?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rips(maxdim=3, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
      "Language:  French\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/ripser/ripser.py:257: UserWarning: The input point cloud has more columns than rows; did you mean to transpose?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rips(maxdim=3, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
      "Language:  Arabic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/ripser/ripser.py:257: UserWarning: The input point cloud has more columns than rows; did you mean to transpose?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rips(maxdim=3, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
      "Language:  Finnish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/ripser/ripser.py:257: UserWarning: The input point cloud has more columns than rows; did you mean to transpose?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rips(maxdim=3, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n",
      "Language:  Hungarian\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/ripser/ripser.py:257: UserWarning: The input point cloud has more columns than rows; did you mean to transpose?\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dgrms = dict()\n",
    "for language in languages:\n",
    "    rips = Rips(maxdim=3)\n",
    "    print(\"Language: \", language)\n",
    "    dgrms[language] = rips.fit_transform(data[language], metric=cosine_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in languages:\n",
    "    plt.figure()\n",
    "    rips.plot(dgrms[lang])\n",
    "    plt.title(lang)\n",
    "#     plt.xlim([0,1])\n",
    "#     plt.ylim([0,1])\n",
    "    plt.savefig('pds_ang_3/' + lang + '.png', dpi=600, bbox_inches = 'tight', pad_inches = 0.2)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree:\n",
    "    def __init__(self, data, left=None, right=None):\n",
    "        self.left = left \n",
    "        self.right = right\n",
    "        self.data = data\n",
    "    def __str__(self):\n",
    "        return \"Data: %s\" % self.data\n",
    "\n",
    "    @staticmethod\n",
    "    def node_list(root):\n",
    "        nlist = []\n",
    "        queue = [root]\n",
    "        while len(queue) > 0:\n",
    "            node = queue.pop()\n",
    "            if node.left:\n",
    "                queue.append(node.left)\n",
    "            if node.right:\n",
    "                queue.append(node.right)\n",
    "            else:\n",
    "                nlist.append(node)\n",
    "        return nlist\n",
    "    \n",
    "    def display(self, keys):\n",
    "        lines, *_ = self._display_aux(keys)\n",
    "        for line in lines:\n",
    "            print(line)\n",
    "\n",
    "    def _display_aux(self, keys):\n",
    "        \"\"\"Returns list of strings, width, height, and horizontal coordinate of the root.\"\"\"\n",
    "        # No child.\n",
    "        if self.right is None and self.left is None:\n",
    "            line = keys[self.data]\n",
    "            width = len(line)\n",
    "            height = 1\n",
    "            middle = width // 2\n",
    "            return [line], width, height, middle\n",
    "\n",
    "        # Only left child.\n",
    "        if self.right is None:\n",
    "            lines, n, p, x = self.left._display_aux(keys)\n",
    "            s = keys[self.data]\n",
    "            u = len(s)\n",
    "            first_line = (x + 1) * ' ' + (n - x - 1) * '_' + s\n",
    "            second_line = x * ' ' + '/' + (n - x - 1 + u) * ' '\n",
    "            shifted_lines = [line + u * ' ' for line in lines]\n",
    "            return [first_line, second_line] + shifted_lines, n + u, p + 2, n + u // 2\n",
    "\n",
    "        # Only right child.\n",
    "        if self.left is None:\n",
    "            lines, n, p, x = self.right._display_aux(keys)\n",
    "            s = keys[self.data]\n",
    "            u = len(s)\n",
    "            first_line = s + x * '_' + (n - x) * ' '\n",
    "            second_line = (u + x) * ' ' + '\\\\' + (n - x - 1) * ' '\n",
    "            shifted_lines = [u * ' ' + line for line in lines]\n",
    "            return [first_line, second_line] + shifted_lines, n + u, p + 2, u // 2\n",
    "\n",
    "        # Two children.\n",
    "        left, n, p, x = self.left._display_aux(keys)\n",
    "        right, m, q, y = self.right._display_aux(keys)\n",
    "        s = keys[self.data]\n",
    "        u = len(s)+1\n",
    "        first_line = (x + 1) * ' ' + (n - x - 1) * '_' + s + y * '_' + (m - y) * ' '\n",
    "        second_line = x * ' ' + '/' + (n - x - 1 + u + y) * ' ' + '\\\\' + (m - y - 1) * ' '\n",
    "        if p < q:\n",
    "            left += [n * ' '] * (q - p)\n",
    "        elif q < p:\n",
    "            right += [m * ' '] * (p - q)\n",
    "        zipped_lines = zip(left, right)\n",
    "        lines = [first_line, second_line] + [a + u * ' ' + b for a, b in zipped_lines]\n",
    "        return lines, n + m + u, max(p, q) + 2, n + u // 2\n",
    "\n",
    "\n",
    "def distance_matrix(nodes, tiny_dmatrix, linkage=\"complete\"):\n",
    "    \"\"\"\n",
    "    Computes distance matrix with complete linkage \n",
    "    \"\"\"\n",
    "    d_matrix = np.full((len(nodes), len(nodes)), np.nan)\n",
    "    for i in range(len(nodes)):\n",
    "        for j in range(i+1, len(nodes)):\n",
    "            nlist_1 = Tree.node_list(nodes[i])\n",
    "            nlist_2 = Tree.node_list(nodes[j])\n",
    "            ids_1 = [node.data for node in nlist_1]\n",
    "            ids_2 = [node.data for node in nlist_2]\n",
    "            if linkage == \"complete\":\n",
    "                d_matrix[i,j] = max([tiny_dmatrix[i,j] for i in ids_1 for j in ids_2])\n",
    "            else:\n",
    "                d_matrix[i,j] = min([tiny_dmatrix[i,j] for i in ids_1 for j in ids_2])\n",
    "    return d_matrix\n",
    "    \n",
    "def hclustering(dgrms, homology=0, dist='sw', linkage=\"complete\"):\n",
    "    nodes = [Tree(i) for i in range(len(dgrms))]\n",
    "    if type(homology) is int:\n",
    "        new_dgrms = [dgrms[i][homology] for i in dgrms]\n",
    "    elif len(homology) == 2:\n",
    "        new_dgrms = [np.vstack((dgrms[i][homology[0]], dgrms[i][homology[1]])) for i in dgrms]\n",
    "    elif len(homology) == 3:\n",
    "        new_dgrms = [np.vstack((dgrms[i][homology[0]], dgrms[i][homology[1]], dgrms[i][homology[2]])) for i in dgrms]\n",
    "    elif len(homology) == 4:\n",
    "        new_dgrms = [np.vstack((dgrms[i][homology[0]], dgrms[i][homology[1]], dgrms[i][homology[2]], dgrms[i][homology[3]])) for i in dgrms]\n",
    "    tiny_dmatrix = np.full((len(nodes), len(nodes)), np.nan)                \n",
    "    for i in range(len(nodes)):\n",
    "        for j in range(i+1, len(nodes)):\n",
    "            if dist == 'sw':\n",
    "                if homology == 0 or (0 in homology):\n",
    "                    temp_i = np.asarray([x for x in new_dgrms[nodes[i].data] if x[1] != np.inf])\n",
    "                    temp_j = np.asarray([x for x in new_dgrms[nodes[j].data] if x[1] != np.inf])\n",
    "                    tiny_dmatrix[i,j] = sliced_wasserstein(temp_i, temp_j)\n",
    "                else:\n",
    "                    tiny_dmatrix[i,j] = sliced_wasserstein(new_dgrms[nodes[i].data], new_dgrms[nodes[j].data])  \n",
    "            else:\n",
    "                tiny_dmatrix[i,j] = bottleneck(new_dgrms[nodes[i].data], new_dgrms[nodes[j].data])\n",
    "            tiny_dmatrix[j,i] = tiny_dmatrix[i,j]\n",
    "    while len(nodes) > 1:\n",
    "        d_matrix = distance_matrix(nodes, tiny_dmatrix,linkage)\n",
    "        i, j = np.unravel_index(np.nanargmin(d_matrix), d_matrix.shape)\n",
    "        print(\"The minimum is \", d_matrix[i,j])\n",
    "        node = Tree(-1, left=nodes[i], right=nodes[j])\n",
    "        nodes = [nodes[k] for k in range(len(nodes)) if k not in [i,j]]\n",
    "        nodes.append(node)        \n",
    "    return nodes[0], tiny_dmatrix\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = list(dgrms.keys())\n",
    "langs.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorter_dgrms = {x:dgrms[x] for x in languages[:-2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum is  1.236719309994832\n",
      "The minimum is  1.8599817414941389\n",
      "The minimum is  2.34496159486684\n",
      "The minimum is  2.724530976196973\n",
      "The minimum is  3.5343840446954444\n",
      "The minimum is  3.980298343539048\n",
      "The minimum is  5.6107845415660105\n",
      "The minimum is  7.617577860481355\n",
      "The minimum is  28.920650722237983\n"
     ]
    }
   ],
   "source": [
    "q1, tiny = hclustering(shorter_dgrms,homology=[0,1,2],dist='sw', linkage=\"complete\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum is  1.262962273017806\n",
      "The minimum is  1.496612227808784\n",
      "The minimum is  1.904786958614355\n",
      "The minimum is  2.5695036379562426\n",
      "The minimum is  2.745530073015718\n",
      "The minimum is  3.283867735969512\n",
      "The minimum is  3.624455040314522\n",
      "The minimum is  4.8713334764250344\n",
      "The minimum is  5.6826285465295685\n",
      "The minimum is  7.617577860481355\n",
      "The minimum is  29.033173526974227\n"
     ]
    }
   ],
   "source": [
    "q1, tiny = hclustering(dgrms,homology=[0,1,2,3],dist='sw', linkage=\"complete\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(shorter_dgrms['English'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     ______________________________________________         \n",
      "                                    /                                               \\        \n",
      "                      ____________________________                              _______    \n",
      "                     /                             \\                            /        \\   \n",
      "       ____________________                ________________               Mandarin Japanese\n",
      "      /                     \\              /                 \\                               \n",
      "    _________            _____         _____          ___________                        \n",
      "   /          \\          /      \\       /      \\        /            \\                       \n",
      "German     ______    Korean Russian Arabic Finnish Hungarian     _____                     \n",
      "          /       \\                                              /      \\                    \n",
      "       Spanish French                                         English Dutch                  \n"
     ]
    }
   ],
   "source": [
    "q1.display(langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_latex_tree(tree,langs):\n",
    "    print(chr(92)+'begin{forest}\\n[',end='')\n",
    "    _make_latex_tree(tree,langs)\n",
    "    print(']\\n'+ chr(92) +'end{forest}',end='')\n",
    "\n",
    "def _make_latex_tree(tree, langs):\n",
    "    if not tree.left and not tree.right:\n",
    "        print(langs[tree.data], end='')\n",
    "    elif tree.data:\n",
    "        print('|[', end='')\n",
    "        _make_latex_tree(tree.left,langs)\n",
    "        print(']',end='')\n",
    "        print('[',end='')\n",
    "        _make_latex_tree(tree.right,langs)\n",
    "        print(']',end='')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{forest}\n",
      "[|[|[|[English][Arabic]][|[|[Spanish][German]][|[Korean][|[Dutch][|[Russian][French]]]]]][|[Mandarin][Japanese]]]\n",
      "\\end{forest}"
     ]
    }
   ],
   "source": [
    "make_latex_tree(q1,langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
